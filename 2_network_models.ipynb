{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'18] milestone 2: network models\n",
    "[ntds'18]: https://github.com/mdeff/ntds_2018\n",
    "\n",
    "[Hermina Petric Maretic](https://people.epfl.ch/hermina.petricmaretic), [EPFL LTS4](https://lts4.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "* Team: `28`\n",
    "* Students: `Guillain, Léonore Valentine; Pase, Francesco; Rusu, Cosmin-Ionut; Zhuang, Ying`\n",
    "* Dataset: `Flight Routes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "* Milestones have to be completed by teams. No collaboration between teams is allowed.\n",
    "* Textual answers shall be short. Typically one to two sentences.\n",
    "* Code has to be clean.\n",
    "* In the first part, you cannot import any other library than we imported. In the second part, you are allowed to import any library you want.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart & Run All\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The purpose of this milestone is to explore various random network models, analyse their properties and compare them to your network. In the first part of the milestone you will implement two random graph models and try to fit them to your network. In this part you are not allowed to use any additional package. In the second part of the milestone you will choose a third random graph model that you think shares some properties with your network. You will be allowed to use additional packages to construct this network, but you must explain your network choice. Finally, make your code as clean as possible, and keep your textual answers short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0\n",
    "\n",
    "Import the adjacency matrix of your graph that you constructed in milestone 1, as well as the number of nodes and edges of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "adjacency = np.load('adjacency.npy') # the adjacency matrix\n",
    "n_nodes =  adjacency.shape[0] # the number of nodes in the network\n",
    "n_edges =  np.where(adjacency > 0, 1, 0).sum() / 2 # the number of edges in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "**For the computation of this part of the milestone you are only allowed to use the packages that have been imported in the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Create a function that constructs an Erdős–Rényi graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erdos_renyi(n, p, seed = None):\n",
    "    \"\"\"Create an instance from the Erdos-Renyi graph model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        Size of the graph.\n",
    "    p: float\n",
    "        Edge probability. A number between 0 and 1.\n",
    "    seed: int (optional)\n",
    "        Seed for the random number generator. To get reproducible results.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adjacency\n",
    "        The adjacency matrix of a graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    adjacency = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # add the +1 to avoid selfloops\n",
    "        adjacency[i, i+1:] = np.random.binomial(1, p, n-i-1)\n",
    "        adjacency[i+1:, i] = adjacency[i, i+1:]\n",
    "    \n",
    "    \n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = erdos_renyi(5, 0.6, 9765)\n",
    "plt.spy(er)\n",
    "plt.title('Erdos-Renyi (5, 0.6)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = erdos_renyi(10, 0.4, 7648)\n",
    "plt.spy(er)\n",
    "plt.title('Erdos-Renyi (10, 0.4)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Use the function to create a random Erdos-Renyi graph. Choose the parameters such that number of nodes is the same as in your graph, and the number of edges similar. You don't need to set the random seed. Comment on your choice of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of probability\n",
    "p = 2*n_edges/(n_nodes*(n_nodes-1))\n",
    "print('Connection propabiblity :  ', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_graph = erdos_renyi(n_nodes, p)\n",
    "plt.spy(er_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Edges in Erdos-Renyi graph : {} \\nEdges in our graph : {}'.format(er_graph.sum()/2, n_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "The number of links in a Erdos-Renyi graph will be $L=p* N(N-1)/2$ for a given $p$, hence we choose $p = 2*L/(N*(N-1))$. We see that the resulting number of edges is indeed similar to the one in our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Create a function that constructs a Barabási-Albert graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barabasi_albert(n, m, m_0 = 2, seed=None,):\n",
    "    \"\"\"Create an instance from the Barabasi-Albert graph model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        Size of the graph.\n",
    "    m: int\n",
    "        Number of edges to attach from a new node to existing nodes.\n",
    "    seed: int (optional)\n",
    "        Seed for the random number generator. To get reproducible results.\n",
    "    m_0: int (optinal)\n",
    "        The number of nodes in the initial connected component\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adjacency\n",
    "        The adjacency matrix of a graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #ensure correct property\n",
    "    if m > m_0:\n",
    "        m_0 = m\n",
    "\n",
    "    # Initial connected network\n",
    "    adjacency = np.zeros((n,n))\n",
    "\n",
    "    \n",
    "    #Generating random subgraph of size m_0 x m_0\n",
    "    adjacency[:m_0, :m_0] = erdos_renyi(m_0, 1./m_0 , seed)\n",
    "\n",
    "    #Force Connectivity\n",
    "    for i in range(m_0-1):\n",
    "        adjacency[i, i+1] = 1;\n",
    "        adjacency[i+1, i] = 1;\n",
    "    \n",
    "    \n",
    "    #iterate:\n",
    "    for i in range(m_0, n):\n",
    "        degrees = adjacency[:i].sum(axis = 1)\n",
    "        total = degrees.sum()\n",
    "        \n",
    "        new_links = np.random.choice(i, size=m, replace=False, p = degrees / total)\n",
    "        adjacency[i, new_links] = 1.\n",
    "        adjacency[new_links, i] = 1.\n",
    "    \n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = barabasi_albert(5, 1, 2, 9087)\n",
    "plt.spy(ba)\n",
    "_ = plt.title('Barabasi-Albert (5, 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = barabasi_albert(10, 2, 3, 8708)\n",
    "plt.spy(ba)\n",
    "_ = plt.title('Barabasi-Albert (10, 2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Use the function to create a random Barabási-Albert graph. Choose the parameters such that number of nodes is the same as in your graph, and the number of edges similar. You don't need to set the random seed. Comment on your choice of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_degree = np.where(adjacency > 0, 1, 0).sum(axis = 1).mean()\n",
    "print('The average degree of our graph is: ', avg_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "ba_graph = barabasi_albert(n_nodes, 6)\n",
    "plt.spy(ba_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of edges in Barabási-Albert graph: {}\\nNumber of edges in our graph: {}\"\n",
    "      .format( ba_graph.sum()/2, n_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n",
    "\n",
    "We calcuate $m$ so that the random Barabási-Albert graph we create has a similar number of edges as our network. The algorithm will create approximately $mt + m_0$ links, with $t= n-m_0$.\n",
    "We obtain $L= m*n-m*m_0+m_0$, as the $m_0 = m$ in our implementation, we get that 6 is the closest integer to give the desired results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Compare the number of edges in all three networks (your real network, the Erdős–Rényi network, and the Barabási-Albert netowk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "print(f\"Number of edges in the real network: {n_edges}\")\n",
    "print(f\"Number of edges in the Erdős–Rényi network: {er_graph.sum() / 2}\")\n",
    "print(f\"Number of edges in the Barabási-Albert network: {ba_graph.sum() / 2}\")\n",
    "\n",
    "plt.title('Number of edges in comparison')\n",
    "plt.ylabel('Nbr of edges')\n",
    "_ = plt.bar(['Our Network', 'Erdős–Rényi', 'Barabási-Albert'], [n_edges, er_graph.sum() / 2, ba_graph.sum() / 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Implement a function that computes the [Kullback–Leibler (KL) divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between two probability distributions.\n",
    "We'll use it to compare the degree distributions of networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute the KL divergence between probability distributions of degrees of two networks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p: np.array\n",
    "        Probability distribution of degrees of the 1st graph.\n",
    "    q: np.array\n",
    "        Probability distribution of degrees of the 2nd graph.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    kl\n",
    "        The KL divergence between the two distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert p.shape == q.shape\n",
    "    #check that we have the same support\n",
    "    np.testing.assert_array_equal(np.where(p>0), np.where(q>0), err_msg='The two distributions have different supports')\n",
    "    \n",
    "    kl = 0\n",
    "    for x, p_x in enumerate(p):\n",
    "        kl += p_x*np.log(p_x/q[x])\n",
    "    \n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = np.array([0.2, 0.2, 0.2, 0.4])\n",
    "q_test = np.array([0.3, 0.3, 0.1, 0.3])\n",
    "kl_divergence(p_test, q_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Compare the degree distribution of your network to each of the two synthetic ones, in terms of KL divergence. **Hint:** Make sure you normalise your degree distributions to make them valid probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "def get_degree_distribution(adjacency):\n",
    "    degrees = adjacency.sum(axis = 1)\n",
    "    #Use histogram with number of bins = degree range to get distribution\n",
    "    degree_network = np.histogram(degrees, bins=int((degrees.max()+1)-degrees.min()))\n",
    "    return degree_network[0], degrees\n",
    "    \n",
    "#calculate degree distrubution for each graph\n",
    "degree_network, degrees = get_degree_distribution(np.where(adjacency > 0, 1, 0))\n",
    "degree_er_network, degrees_er = get_degree_distribution(er_graph)\n",
    "degree_ba_network, degrees_ba = get_degree_distribution(ba_graph)\n",
    "\n",
    "#use to pad later\n",
    "deg= np.array([degrees, degrees_er, degrees_ba])\n",
    "min_s = deg.min(axis=1)\n",
    "max_s = deg.max(axis=1)\n",
    "degree_networks_ = [degree_network, degree_er_network, degree_ba_network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad arrays to have same length\n",
    "def pad_distribution(degree_network, degrees ,min_s,max_s):\n",
    "    mi = degrees.min()\n",
    "    ma = degrees.max()\n",
    "    before_pad = int(mi - min(min_s))\n",
    "    after_pad = int( max(max_s) - ma)\n",
    "    return np.pad(degree_network, pad_width=(before_pad, after_pad), mode='constant', constant_values = 0)\n",
    "    \n",
    "for i, d in enumerate(deg):\n",
    "    degree_networks_[i] = pad_distribution(degree_networks_[i], d, min_s, max_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "#normalize\n",
    "def normalize(degree_networks):\n",
    "    tmp = []\n",
    "    for n in degree_networks:\n",
    "\n",
    "        #to avoid issues with log at zero, affects accuracy only in order of 1e-3\n",
    "        n += 1\n",
    "\n",
    "        #alternative smoothing, we still would have issues with zero\n",
    "        #we find moving average to be sufficient\n",
    "        #alternative smoothing, we still would have issues with zero\n",
    "        #scp = scipy.interpolate.UnivariateSpline(range(min(min_s), max(max_s)+1), n)\n",
    "        #n = scp(range(min(min_s), max(max_s)+1))\n",
    "\n",
    "        #do moving average to smooth function slightly\n",
    "        N = n.shape[0]\n",
    "        n = running_mean(n, 3)\n",
    "        n = n/ n.sum()\n",
    "        tmp.append(n)\n",
    "    return tmp\n",
    "    \n",
    "degree_networks = normalize(degree_networks_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"KL Divergence between the original network and the Erdős–Rényi network: {kl_divergence(degree_networks[0], degree_networks[1])}\")\n",
    "print(f\"KL Divergence between the original network and the Barabási-Albert network: {kl_divergence(degree_networks[0], degree_networks[2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we look only at degrees >= 5\n",
    "tmp = []\n",
    "copy = degree_networks_.copy()\n",
    "\n",
    "for n in copy:\n",
    "    n = n[5:]\n",
    "    n = n/ n.sum()\n",
    "    tmp.append(n)\n",
    "    \n",
    "degree_networks_2 = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"KL Divergence between the original network and the Erdős–Rényi network for degrees >= 5: \\\n",
    "{kl_divergence(degree_networks_2[0], degree_networks_2[1])}\")\n",
    "print(f\"KL Divergence between the original network and the Barabási-Albert network for degrees >= 5: \\\n",
    "{kl_divergence(degree_networks_2[0], degree_networks_2[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Plot the degree distribution historgrams for all three networks. Are they consistent with the KL divergence results? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scatter_histogram(degree_dist, title='Degree distribution of our network', alpha=1, ylim = (0.0001, 0.3)):\n",
    "    p = plt.scatter(range(len(degree_dist)), degree_dist, alpha=alpha)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('probability')\n",
    "    plt.ylim(ylim)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.suptitle('Degree distribution of our network ')\n",
    "\n",
    "plt.subplot(132)\n",
    "custom_scatter_histogram(degree_networks[0])\n",
    "plt.title('Log scatter histogram')\n",
    "plt.ylabel('probability')\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Conventional log histogram')\n",
    "plt.hist(np.where(adjacency > 0, 1, 0).sum(axis=1), log=True)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Normalized degree distribution')\n",
    "plt.plot(degree_networks[0])\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('probability')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.suptitle('Degree distribution of Erdős–Rényi network')\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Conventional log histogram')\n",
    "plt.hist(er_graph.sum(axis=1), log=True)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(132)\n",
    "custom_scatter_histogram(degree_networks[1])\n",
    "plt.title('Log scatter histogram')\n",
    "plt.ylabel('probability')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Normalized degree distribution')\n",
    "plt.plot(degree_networks[1])\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('probability')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.suptitle('Degree distribution of Barabási-Albert network')\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Conventional log histogram')\n",
    "plt.hist(ba_graph.sum(axis=1), log=True)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.subplot(132)\n",
    "custom_scatter_histogram(degree_networks[2])\n",
    "plt.title('Log scatter histogram')\n",
    "plt.ylabel('probability')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Normalized degree distribution')\n",
    "plt.plot(degree_networks[2])\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('probability')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('normalized degree distributions')\n",
    "for i in degree_networks:\n",
    "    plt.plot(i)\n",
    "    \n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend(['Our network', ' Erdős–Rényi', 'Barabási-Albert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "plt.title('Comparing (non normalized) degree distribution histogram')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Nbr of Occurences')\n",
    "plt.hist(np.where(adjacency > 0, 1, 0).sum(axis = 1), alpha=0.4, bins=100)\n",
    "plt.hist(er_graph.sum(axis=1), alpha=0.4, log=True)\n",
    "plt.hist(ba_graph.sum(axis=1), alpha=0.4, bins=100)\n",
    "plt.legend(['Our network', 'Erdős–Rényi', 'Barabási-Albert'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Comparing degree distribution - scatter histogram')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Nbr of Occurences')\n",
    "for i in degree_networks:\n",
    "    custom_scatter_histogram(i, alpha=0.6)\n",
    "    \n",
    "    \n",
    "plt.legend(['Our network', ' Erdős–Rényi', 'Barabási-Albert'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "We can clearly see that the overall shape of the Barabási-Albert degree distribution is more similar to our model than the Erdős–Rényi degree distribution. The KL-values over the whole distribution do not reflect this. \n",
    "By removing the first 5 values, we see that the KL for this now reflect that Barabási-Albert model is indeed more similar to our model. The offset in small degree values (no nodes of degree <m ) in Barabási-Albert model causes this discrepancy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Imagine you got equal degree distributions. Would that guarantee you got the same graph? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have the same degree distrubtion, this does not mean that we have the same graph. \n",
    "\n",
    "If two graphs have same degree sequence, that does not mean that the graphs are isomorphic, hence they may not be the same. \n",
    "\n",
    "To prove this, consider the following counter example: \n",
    "These two have the same degree distribution but are not isomorphic : {1: 2, 2: 3}\n",
    "\n",
    "```\n",
    "* 1 -- 2 -- 3 -- 4 -- 5\n",
    "* 1 -- 2   3 -- 4\n",
    "             \\ /\n",
    "              5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "**You are allowed to use any additional library here (e.g., NetworkX, PyGSP, etc.).** Be careful not to include something here and use it in part 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Choose a random network model that fits you network well. Explain your choice. \n",
    "\n",
    "**Hint:** Check lecture notes for different network models and their properties. Your choice should be made based on at least one property you'd expect to be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous exercise we see that our network follows a power law distribution. As we saw in class such distributions emerge whenever both growth and preferential attachment come into play. \n",
    "Moreover, not only do these comonents give raise mathematically to the desired distribution, but they also model the behaviour that gave rise to the real network.\n",
    "\n",
    "Very popular airports represent economical and social hubs in our society. As a consequence, they are more likely to get new links (addition of a new direct flight) whenever new companies and/or airports join the network. We chose then to reproduce a network with a similar degree distribution shape, maintaining the same number of nodes and similar number of edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Explain (in short) how the chosen model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "As we can see above, the Barabasi Albert model with similar number of edges as our network has smaller hubs than ours. In the above Barabasi Albert model, preferential attchment ($\\alpha$) is equal to $1$, meaning that the probability for a new node to create a link with older nodes is linear in the node degree. \n",
    "\n",
    "The occurrence of larger hubs in our network indicates that we need a model that can generate networks with super linear preferential attachment.\n",
    "In order to come up with such model we decided to slightly modify the Barabasi-Albert algorithm. Another consideration refers to low degree nodes: in order to get more low degree nodes (as we have in our network) our model starts with high value for $m_0$ (initial poorly connected nodes).\n",
    "\n",
    "The algorithm is fairly simple:\n",
    "\n",
    "1. Create a single connected component of $m_0$ nodes.\n",
    "2. For every new node (until we reached the number of nodes we want):\n",
    "    1. Create links to ($m <<= m_0$) already existing nodes with probabilities proportional to the degree of the existing node elevated at some esponent $\\alpha$, i.e.\n",
    "    $$p_i = \\frac{degree_i^{\\alpha}}{\\sum_{j \\in S}{degree_j^{\\alpha}}} \\text{ where } S \\text{ is the set of existing nodes} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "Create a random graph from that model, such that the number of nodes is the same as in your graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preferential_attachment(n, m, seed=None, m_0 = None, alpha = 1):\n",
    "    \"\"\"Create an instance from the Barabasi-Albert graph model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        Size of the graph.\n",
    "    m: int\n",
    "        Number of edges to attach from a new node to existing nodes.\n",
    "    seed: int (optional)\n",
    "        Seed for the random number generator. To get reproducible results.\n",
    "    m_0: int (optinal)\n",
    "        The number of nodes in the initial connected component\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adjacency\n",
    "        The adjacency matrix of a graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    # Initial connected network\n",
    "    adjacency = np.zeros((n,n))\n",
    "    \n",
    "\n",
    "    \n",
    "    adjacency[:m_0, :m_0] = erdos_renyi(m_0, 1./(10*m_0) , seed)\n",
    "    #Should it also be connected ??\n",
    "    for i in range(m_0-1):\n",
    "        adjacency[i, i+1] = 1;\n",
    "        adjacency[i+1, i] = 1;\n",
    "    #iterate:\n",
    "    for i in range(m_0, n):\n",
    "        degrees = adjacency[:i].sum(axis = 1)\n",
    "        total = (degrees**alpha).sum()\n",
    "        \n",
    "        new_links = np.random.choice(i, size=m, replace=False, p = degrees**alpha / total)\n",
    "        adjacency[i, new_links] = 1.\n",
    "        adjacency[new_links, i] = 1.\n",
    "    \n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values were heuristically to tuned to obtain a fair representation of our netowrk\n",
    "preferential = preferential_attachment(n_nodes, m=7, m_0 = 500, alpha = 1.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our network number of nodes: {n_nodes} \\nEdges: {int(n_edges)}\")\n",
    "print(f\"New random network number of nodes: {preferential.shape[0]} \\nEdges: {int(preferential.sum() / 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 13\n",
    "\n",
    "Check the properties you expected to be similar, and compare to your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "one_adjacency = np.where(adjacency > 0, 1, 0)\n",
    "x = one_adjacency.sum(axis = 1)\n",
    "degree_pref_network = preferential.sum(axis = 1)\n",
    "\n",
    "\n",
    "plt.title('Degree distribution for our random network: superlinear attachment')\n",
    "plt.hist(degree_pref_network, bins = 237, alpha=0.4, log = True)\n",
    "plt.hist(x, bins = 237, alpha=0.4, log = True)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(['Random network with superlinear pref. attachment',' Our network'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulating the distributions to compute KL Divergence and compare the networks\n",
    "de, pref_ = get_degree_distribution(preferential)\n",
    "degree_network, degrees = get_degree_distribution(np.where(adjacency > 0, 1, 0))\n",
    "\n",
    "deg= np.array([degrees, pref_])\n",
    "min_s = deg.min(axis=1)\n",
    "max_s = deg.max(axis=1)\n",
    "degree_networks_ = [degree_network, de]\n",
    "\n",
    "for i, d in enumerate(deg):\n",
    "    degree_networks_[i] = pad_distribution(degree_networks_[i], d, min_s, max_s)\n",
    "    \n",
    "degree_networks = normalize(degree_networks_)\n",
    "\n",
    "    \n",
    "for i in degree_networks:\n",
    "    custom_scatter_histogram(i, alpha=0.6, title='Plots in comparison')\n",
    "    \n",
    "plt.legend(['Our Network', 'Random network with superlinear pref. attachment'])\n",
    "\n",
    "print(f\"KL Divergence between the original network and the Random network with superlinear preferential attachment:\\n {kl_divergence(degree_networks[0], degree_networks[1])}\")\n",
    "\n",
    "print(f\"Average mean Random network with superlinear preferential attachment: {pref_.mean()}\\n\\\n",
    "Average mean in our Network: {degrees.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "We can see that the values of the their average degree are very closed indeed from the plot it is possible to observe the two distributions are very similar. In order to quantitatively justify such observation we also computed the KL Divergence: it has more than halved with respect to those of both models presented in Part 1. \n",
    "\n",
    "Actually the point were the two distributions differ the most is again in the low degree nodes. It is indeed quite difficult to obtain such preferential attachment with a growing model that ends up having many low degree nodes as in our network. This is the consequence of the algorithm: on the one hand it has to attach a fixed amount of links for every new entering node in order to obtain big hubs and a certain amount of edges; on the other hand, it should allow for many nodes to have very low degree (one or two). In the model above we tried to heuristically tune $\\textit{$m_0$}, m$ and $\\textit{alpha}$ that better fit our network. That ended up again in having a distribution that was slightly shifted towards higher degrees in the first entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Networkx tool}$\n",
    "\n",
    "To solve the issue explained above, we looked for different generating methods in the networkx libraries. The problem remained unsolved for the growing models presented in such library. As example, we tested both $\\textit{scale_free_graph()}$ and $\\textit{powerlaw_cluster_graph()}$ but they behaved as our $\\textit{preferential_attachment()}$ model. They all are indeed modified version of the Barabasi-Albert algorithm. Given that, we decided to test the method $\\textit{expected_degree_graph()}$ that generates random networks starting from the array representing, for every nodes, its degree. The model does not make use of \"growth\" but it just creates a link between two nodes $i$ and $j$ with probability:\n",
    "\n",
    "$$ p_{ij} = \\frac{k_ik_j}{\\sum_{t}{k_t}}, \\text{ where } k_t \\text{ is the degree of node $t$}. $$\n",
    "\n",
    "Thus this model generates a random network having a very similar distribution of the original one. Moreover, taking as connection proboabilities such function, it makes use of preferential attachment as we want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "random_graph = nx.expected_degree_graph(list(degrees), selfloops=False)\n",
    "#Create degrees vector\n",
    "degree_random = [d for n, d in random_graph.degree()]\n",
    "\n",
    "\n",
    "print(f\"Our network number of nodes: {n_nodes} and edges: {int(n_edges)}\")\n",
    "print(f\"New model network number of nodes: {random_graph.number_of_nodes()} and edges: {random_graph.number_of_edges()}\")\n",
    "\n",
    "plt.title('Degree distribution for our random network')\n",
    "plt.hist(degree_random, bins = 237, alpha=0.4, log = True)\n",
    "\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "plt.title('Comparison degree distribution histograms')\n",
    "plt.hist(degree_random, alpha=0.4, log=True, bins=237)\n",
    "\n",
    "plt.hist(np.where(adjacency > 0, 1, 0).sum(axis=1), alpha=0.4, bins=237)\n",
    "plt.legend(['Random network', 'Our network'])\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Nbr of Occurences')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_network_random_new, degree_random_new  = get_degree_distribution(nx.to_numpy_array(random_graph))\n",
    "degree_network, degrees = get_degree_distribution(one_adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg= np.array([degrees, degree_random_new])\n",
    "min_s = deg.min(axis=1)\n",
    "max_s = deg.max(axis=1)\n",
    "degree_networks_ = [degree_network, degree_network_random_new]\n",
    "\n",
    "for i, d in enumerate(deg):\n",
    "    degree_networks_[i] = pad_distribution(degree_networks_[i], d, min_s, max_s)\n",
    "    \n",
    "\n",
    "#normalize to view better dist\n",
    "    \n",
    "degree_networks = normalize(degree_networks_)\n",
    "\n",
    "    \n",
    "for i in degree_networks:\n",
    "    custom_scatter_histogram(i, alpha=0.6, title='Scatter histograms in comparison')\n",
    "    \n",
    "plt.legend(['Random network', 'Our network'])\n",
    "\n",
    "print(f\"KL Divergence between the original network and the random network: \\\n",
    "{kl_divergence(degree_networks[0], degree_networks[1])}\")\n",
    "\n",
    "\n",
    "print(f\"Average mean Random network with superlinear preferential attachment: {degree_random_new.mean()}\\n\\\n",
    "Average mean in our Network: {degrees.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "We can see that now the two distributions overlap in nearly all points and the KL Divergence is very small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
