{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'18] milestone 3: spectral graph theory\n",
    "[ntds'18]: https://github.com/mdeff/ntds_2018\n",
    "\n",
    "[Michaël Defferrard](http://deff.ch), [EPFL LTS2](https://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "* Team: `28`\n",
    "* Students: `Guillain, Léonore Valentine; Pase, Francesco; Rusu, Cosmin-Ionut; Zhuang, Ying`\n",
    "* Dataset: `Flight Routes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "* Milestones have to be completed by teams. No collaboration between teams is allowed.\n",
    "* Textual answers shall be short. Typically one to two sentences.\n",
    "* Code has to be clean.\n",
    "* You cannot import any other library than we imported.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart & Run All\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this milestone is to get familiar with the graph Laplacian and its spectral decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Load your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a `No module named 'sklearn'` error when running the below cell, install [scikit-learn](https://scikit-learn.org) with `conda install scikit-learn` (after activating the `ntds_2018` environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote your graph as $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, A)$, where $\\mathcal{V}$ is the set of nodes, $\\mathcal{E}$ is the set of edges, $A \\in \\mathbb{R}^{N \\times N}$ is the (weighted) adjacency matrix, and $N = |\\mathcal{V}|$ is the number of nodes.\n",
    "\n",
    "Import the adjacency matrix $A$ that you constructed in the first milestone.\n",
    "(You're allowed to update it between milestones if you want to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = np.load('adjacency.npy') # the adjacency matrix\n",
    "n_nodes =  adjacency.shape[0] # the number of nodes in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "From the (weighted) adjacency matrix $A$, compute both the combinatorial (also called unnormalized) and the normalized graph Laplacian matrices.\n",
    "\n",
    "Note: if your graph is weighted, use the weighted adjacency matrix. If not, use the binary adjacency matrix.\n",
    "\n",
    "For efficient storage and computation, store these sparse matrices in a [compressed sparse row (CSR) format](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR.2C_CRS_or_Yale_format.29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_laplacian(adjacency):\n",
    "    \"\"\"Computes the laplacian given the adjacency matrix\"\"\"\n",
    "    degrees = adjacency.sum(axis=1)\n",
    "    \n",
    "    #Compute D and D^-1/2, as D is diagonal we can just use element wise computation\n",
    "    D = np.diag(degrees)\n",
    "    D_inv_sqrt = scipy.sparse.csc_matrix(np.diag(1 / (np.sqrt(degrees))))\n",
    "\n",
    "    #Calculate the different laplacians\n",
    "    laplacian_combinatorial =  scipy.sparse.csr_matrix(D - adjacency)\n",
    "    laplacian_normalized =  scipy.sparse.csr_matrix.dot(D_inv_sqrt, scipy.sparse.csr_matrix.dot(laplacian_combinatorial, D_inv_sqrt))\n",
    "    \n",
    "    return laplacian_normalized, laplacian_combinatorial, D_inv_sqrt\n",
    "    \n",
    "laplacian_normalized, laplacian_combinatorial, D_inv_sqrt = compute_laplacian(adjacency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of them as the graph Laplacian $L$ for the rest of the milestone.\n",
    "We however encourage you to run the code with both to get a sense of the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian =  laplacian_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Compute the eigendecomposition of the Laplacian $L = U^\\top \\Lambda U$, where the columns $u_k \\in \\mathbb{R}^N$ of $U = [u_1, \\dots, u_N] \\in \\mathbb{R}^{N \\times N}$ are the eigenvectors and the diagonal elements $\\lambda_k = \\Lambda_{kk}$ are the corresponding eigenvalues.\n",
    "\n",
    "Make sure that the eigenvalues are ordered, i.e., $0 = \\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We compare different methods & choose the most appropoiate one\n",
    "%timeit np.linalg.eig(laplacian.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit scipy.linalg.eig(laplacian.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes way too long to run, +2 minutes for a single execution\n",
    "#%timeit scipy.sparse.linalg.eigs(laplacian.asfptype(), k=n_nodes-2)\n",
    "#matrix is symetric\n",
    "print(f'Our matrix is symetric: {np.allclose(adjacency, adjacency.T)}')\n",
    "#%timeit scipy.sparse.linalg.eigsh(laplacian.asfptype(), n_nodes-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.linalg.eigh(laplacian.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit scipy.linalg.eigh(laplacian.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors =  np.linalg.eigh(laplacian.toarray())\n",
    "\n",
    "#sanity check\n",
    "assert eigenvectors.shape == (n_nodes, n_nodes)\n",
    "\n",
    "#sort the eigenvalues\n",
    "indexes_sorted = np.argsort(eigenvalues)\n",
    "eigenvalues_sorted = np.sort(eigenvalues)\n",
    "eigenvector_sorted = eigenvectors[:,indexes_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justify your choice of eigensolver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "We use the numpy eigensolver <span style='color:red'>**np.linalg.eigh()**</span> for symetric matrixes (using the dense matrix) because it is the fastest and has the most stable results, as can be seen above. Additionaly scipy.sparse's implementatios <span style='color:blue'>scipy.sparse.linalg.eigs()</span> and <span style=\"color:blue\">scipy.sparse.linalg.eigsh()</span> are not able compute all the $\\textit{n_nodes}$ eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "We can write $L = S S^\\top$. What is the matrix $S$? What does $S^\\top x$, with $x \\in \\mathbb{R}^N$, compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "The matrix $S \\in \\mathbb{R}^{N \\times M}$, where $M = |\\mathcal{E}|$ and $N = |\\mathcal{V}|$, is the Incidence Matrix of a (directed) graph, whos entries are defined to be: \n",
    "\n",
    " \\begin{equation}\n",
    "    S(i,j) =\n",
    "    \\begin{cases}\n",
    "      +1, & \\text{if}\\ \\exists k : e_j = (v_i, v_k) \\in \\mathcal{E} \\\\\n",
    "      -1, & \\text{if}\\ \\exists k : e_j = (v_k, v_i) \\in \\mathcal{E} \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  \\end{equation}\n",
    "  \n",
    "If we think of $\\vec{x} \\in \\mathbb{R}^N$ as a function defined in the graph space (every vertex of the graph is a point of the Domain), the value $S^\\top \\vec{x} \\in \\mathbb{R}^M$ denotes the gradient of $\\vec{x}$ over the $M$ edges. \n",
    "\n",
    "\n",
    "Indeed $$(S^\\top \\vec{x})_j = \\sum_h S^\\top(j, h) \\cdot x_h =  x_i - x_k$$ —where pedix $h$ denotes the the $h$-th element of a vector and j is the edge connecting node $i$ to $k$— is the difference of the function computed at two points that are close to each other on the domain. Hence, giving the gradient of $\\vec{x}$. The direction of the gradient is given by the direction of the edge connecting the nodes ($\\textit{oriented Incidence Matrix}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Show that $\\lambda_k = \\| S^\\top u_k \\|_2^2$, where $\\| \\cdot \\|_2^2$ denotes the squared Euclidean norm (a.k.a. squared $L^2$ norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "We can easily see that:\n",
    "\n",
    "$$\n",
    " \\| S^\\top u_k \\|_2^2 \\stackrel{(1)}{=} < (S^\\top u_k)^T, S^\\top u_k >\\stackrel{(2)}{=} < u_k^T S, S^\\top u_k > \\stackrel{(3)}{=} u_k^T SS^\\top u_k \\stackrel{(4)}{=} u_k^T L u_k \\stackrel{(5)}{=} \\lambda_k u_k^Tu_k \\stackrel{(6)}{=} \\lambda_k\n",
    "$$\n",
    "\n",
    "- (1) by definition of Euclidian norm\n",
    "- (2) by property of Transpose\n",
    "- (3) by property of Inner product\n",
    "- (4) by property of Laplacian\n",
    "- (5), (6) the eigenvectors decomposition finds orthonormal vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the quantity $\\| S^\\top x \\|_2^2$ tell us about $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "By the definition of the Euclidian norm and our answer in question 3 we have $\\| S^\\top x \\|_2^2 = \\sum_{i\\sim k}(x_i - x_k)^2$ . This is a measure of the smoothness of the function $x$. We note that, if the graph is weighted, we have: $\\| S^\\top x \\|_2^2 = \\sum_{i\\sim k}w_{i,k}(x_i - x_k)^2$, where $w_{i,k}$ is the weight of the edge $j$. Given this, $\\| S^\\top x \\|_2^2 = x^TLx$ accumulates all the squared variations of the function (Laplacian operator in the graph domain) along all the edges (positive values). Thus giving an indication of how fast the function varies over the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What is the value of $u_0$, both for the combinatorial and normalized Laplacians?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We multiply u0 by D^(-1/2) to level all the nonzero elements of the vector\n",
    "plt.scatter(np.arange(eigenvalues.shape[0]), D_inv_sqrt.toarray() @ eigenvector_sorted[:,1])\n",
    "plt.title(\"First eigenvector (Rescaled)\")\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n",
    "\n",
    "#non leveled eigenvector\n",
    "plt.scatter(np.arange(eigenvalues.shape[0]), eigenvector_sorted[:,1])\n",
    "plt.title(\"First eigenvector\")\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Value\")\n",
    "print(\"Number of nonzero elements in the first eigenvector :\", sum(eigenvector_sorted[:,1]<-0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in class, the first eigenvector $\\textbf{u}_0$ of the combinatorial laplacian lies along the direction of the vector $\\textbf{1}$ (it is not exactly the $\\textbf{one}$ vector because of the eigensolver) if the network presents a single connected components. For the normalized laplacian, it is the same vector where each component is rescaled by the squared root of the degree of such component (node). In our case, given that our network is not a single connected component, $\\textbf{u}_0$ is an indicator function indicating the nodes belonging to one of the connected components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Look at the spectrum of the Laplacian by plotting the eigenvalues.\n",
    "Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( eigenvalues_sorted)\n",
    "plt.title(\"Eigenvalues sorted\")\n",
    "plt.xlabel(\"Rank\")\n",
    "_ = plt.ylabel(\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(eigenvalues_sorted, histtype='step', bins=100, log=False)\n",
    "plt.title(\"Density of spectrum of the Laplacian\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Counts\")\n",
    "#It should be equals to the number of nodes, and it is\n",
    "print(\"Sum of eigenvalues is :\" , np.sum(eigenvalues_sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "For the normalized laplacian, the sum of the eigenvalues is equal to the number of nodes in the graph, consequently the distribution of such values has this constraint. The density of the spectrum is very interesting because it reveals about the sctructure of our graph. The shape of the density is an indication that our network is scale-free, as described in [paper link](http://ceec.fnts.bg/telecom/2017/documents/CD2017/Papers/9.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many connected components are there in your graph? Answer using the eigenvalues only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We scatter the first 10 to see the connected components\n",
    "plt.scatter(np.arange(10), eigenvalues_sorted[0:10])\n",
    "plt.title(\"First 10 eigenvalues\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.xlabel(\"Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "In our network we have 7 connected components. We can infer this by looking at the algebraic multiplicity of the eigenvalues zero that's equal to the number of connected components. This is true for both the matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there an upper bound on the eigenvalues, i.e., what is the largest possible eigenvalue? Answer for both the combinatorial and normalized Laplacians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "For both the matrices the minimum value is zero, as the matrices are positive semidefinite. Then, given that we used the normalized laplacian, the largest possible eigenvalue is 2 (it is equal to 2 if the graph is bipartite). For the combinatorial case, the largest eigenvalue is clearly upper bounded by the fact that, for any matrix, the sum of the eigenvalues is equal to the Trace of the matrix. For the laplacian matrices, this is twice the number of edges of the graph. Reasearching we found that as a consequence of the [Gershgorin's Theorem](https://en.wikipedia.org/wiki/Gershgorin_circle_theorem) the largest eigenvalue $\\lambda_n$ is at most twice the maximum vertex degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Laplacian eigenmaps\n",
    "\n",
    "*Laplacian eigenmaps* is a method to embed a graph $\\mathcal{G}$ in a $d$-dimensional Euclidean space.\n",
    "That is, it associates a vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$.\n",
    "The graph $\\mathcal{G}$ is thus embedded as $Z \\in \\mathbb{R}^{N \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What do we use Laplacian eigenmaps for? (Or more generally, graph embeddings.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "It is used to perform dimensionality reduction: go from high dimensional datapoints to a lower dimensional equivalent, while preserving similarities (locality preservation) among the datapoints. In this sense, this technique is mostly used to better visualize graphs in a low-dimensional space and to detect structures in the datasets from which graphs are derived.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Embed your graph in $d=2$ dimensions with Laplacian eigenmaps.\n",
    "Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.\n",
    "\n",
    "**Recompute** the eigenvectors you need with a partial eigendecomposition method for sparse matrices.\n",
    "When $k \\ll N$ eigenvectors are needed, partial eigendecompositions are much more efficient than complete eigendecompositions.\n",
    "A partial eigendecomposition scales as $\\Omega(k |\\mathcal{E}|$), while a complete eigendecomposition costs $\\mathcal{O}(N^3)$ operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate largest component\n",
    "eig_v = (D_inv_sqrt.toarray() @ eigenvector_sorted)[:,0:7]\n",
    "mean = np.median(eig_v, axis=0)\n",
    "indixes = np.where(abs(eig_v-mean) < 1e-10, 1, 0)\n",
    "\n",
    "mask_largest_component = indixes.T[np.argmin(indixes.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recomputing eigendecomposition\n",
    "laplacian_normalized_, laplacian_combinatorial_, D_inv_sqrt_ = compute_laplacian(\n",
    "    adjacency[mask_largest_component > 0][:, mask_largest_component > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the nodes embedded in 2D. Comment on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EIGHS SOLVER\n",
    "eigen_val, eigen_vec = scipy.sparse.linalg.eigsh(\n",
    "    scipy.sparse.csc_matrix(laplacian_normalized_).asfptype(), 20, which='SM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter( eigen_vec[:,0], eigen_vec[:,1], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "We can observe that most of the nodes have similar positions leading to overlap in the plot, indicating that our network is pretty dense. We see also very few outlier nodes. The \"L\" shape of our eigenmap shows that most nodes have zero along the Y-axis; then we have some nodes that have non-zeros along Y-axis and small values along the X-axis. Thus we hypothesis that our network can be split into 2 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the embedding $Z \\in \\mathbb{R}^{N \\times d}$ preserve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "\n",
    "As stated before, the embedding procedure preserves local similarity between points mapping linking similarities (graph topology) to euclidean proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Spectral clustering\n",
    "\n",
    "*Spectral clustering* is a method to partition a graph into distinct clusters.\n",
    "The method associates a feature vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$, then runs [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering) in the embedding space $\\mathbb{R}^d$ to assign each node $v_i \\in \\mathcal{V}$ to a cluster $c_j \\in \\mathcal{C}$, where $k = |\\mathcal{C}|$ is the number of desired clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Choose $k$ and $d$. How did you get to those numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(20), eigen_val[0:20])\n",
    "plt.title(\"First 20 eigenvalues of the biggest component\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "$k$ is the number of clusters we will get, $d$ is the dimensionality of our datapoints. We chose $d$ as to maximize the gap between two consecutive eigenvalues. As we can see on the above plot we have smooth transitions among the eigenvalues but 6 and 2 have the largest gaps, with 6 seeming slightly larger. Hence we choose 6.\n",
    "\n",
    "\n",
    "As we have no ground truth and do thus not know how many clusters we should have, we have no choice but to heuristically find a k that gives us \"good\" clusters. By looking at the plot in 2-dimension, we decided to split our graph into 2 components (X and Y axis), and we later see that using k=5 also gives good, but different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "1. Embed your graph in $\\mathbb{R}^d$ as $Z \\in \\mathbb{R}^{N \\times d}$.\n",
    "   Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.\n",
    "1. If you want $k=2$ clusters, partition with the Fiedler vector. For $k > 2$ clusters, run $k$-means on $Z$. Don't implement $k$-means, use the `KMeans` class imported from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want 2 clusters, so we use the Fiedler Vector\n",
    "labels = np.where(eigen_vec[:,1] < 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative, gives different results!\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "# Normalized vectors and Fitting the input data\n",
    "kmeans = kmeans.fit(eigen_vec[:,:7])\n",
    "# Getting the cluster labels\n",
    "labels_kmeans = kmeans.predict(eigen_vec[:,:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how many nodes are in each cluster\n",
    "d = {}\n",
    "for label in labels:\n",
    "    if label not in d.keys():\n",
    "        d[label] = 1\n",
    "    else :\n",
    "        d[label] += 1\n",
    "print(\"Size of Clusters: \", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "Use the computed cluster assignment to reorder the adjacency matrix $A$.\n",
    "What do you expect? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we sort the list (label, index of node) and the retrive the index\n",
    "L = [ (labels_kmeans[i],i) for i in np.arange(len(labels_kmeans)) ]\n",
    "L.sort()\n",
    "reorder = [i for l, i in L]\n",
    "plt.spy(adjacency[reorder][:, reorder], markersize=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "We expect to see a quasi-diagonal matrix, but given the density of our components it is not easy to isolate good clusters. We can observe a big block component along the diagonal that represents the most connected part of our network (with our hubs). Moreover, within such block we can observe almost 3 pretty dense groups of nodes. On the lower-right corner of the matrix instead we can see the poorly connected nodes connected to the biggest block with just few links.\n",
    "\n",
    "(We note that both reordering by the fiedler vector clusters and 5-means clusters gives us nearly identical adjacency )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "If you have ground truth clusters for your dataset, compare the cluster assignment from spectral clustering to the ground truth.\n",
    "A simple quantitative measure is to compute the percentage of nodes that have been correctly categorized.\n",
    "If you don't have a ground truth, qualitatively assess the quality of the clustering.\n",
    "\n",
    "Ground truth clusters are the \"real clusters\".\n",
    "For example, the genre of musical tracks in FMA, the category of Wikipedia articles, the spammer status of individuals, etc.\n",
    "Look for the `labels` in the [dataset descriptions](https://github.com/mdeff/ntds_2018/tree/master/projects/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n",
    "\n",
    "While we don't have a ground truth label, our nodes correspond to airports and thus are embedded on a 2D hyperplane in 3D. We check if our clustering corresponds to something on the map by plotting the physical location of the nodes. This gives us an intuitive way to qualitatively evaluate the clusters, as we can use our knowledge of geography to infer if the cluster is \"meaningful\" in some way or not.\n",
    "\n",
    "As we can see, we were able to isolate airports placed in the north-west of the United States together with Alaska's airports. This could suggest that in order to reach Alaska we should stop in one of the US Airports highlighted in our map. \n",
    "\n",
    "The alternative clustering using k-mean with k=5 is also evaluated here. The results are interesting as well, as they cluster together \"hard to get to\" places that are close in location. \n",
    "\n",
    "However, the cluster does not isolate all \"isolated\" locations, only ones close to north america. We suspect that there should be more clusters (as there are more isolated nodes, also see Q15), but increasing k-leads to very spread out and random looking clusters. Hence using K-means does give us some interesting clusters, but it can not isolate all the ones we would expect to see (consider pacific islands for instance).\n",
    "\n",
    "Additionaly we note that clustering by the Fiedler vector is able to give a more equally sized split and it gives the two different directions highlighted in the 2D embedding, which is not the case for k means clustering (Question 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features = pd.read_csv(\"features.csv\", index_col=0)\n",
    "locations = np.array(features[mask_largest_component > 0].apply(lambda x : [x.Latitude, x.Longitude], axis=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Partitioning using the Fiedler vector')\n",
    "_ = plt.scatter(locations[:,1], locations[:,0], alpha=0.9, cmap=plt.cm.coolwarm, c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('K-means clustering : k=5')\n",
    "_ = plt.scatter(locations[:,1], locations[:,0], alpha=0.9, cmap=plt.cm.tab20, c=labels_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "Plot the cluster assignment (one color per cluster) on the 2D embedding you computed above with Laplacian eigenmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move code from above here\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(eigen_vec[:,0], eigen_vec[:,1], alpha=0.5, cmap=plt.cm.Set1, c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "\n",
    "Why did we use the eigenvectors of the graph Laplacian as features? Could we use other features for clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "\n",
    "We use the row components of the eigenvectors as features for the clustering because points that are close to each other in the eigenmaps representation have strong connections to each other in the graph representaion, and so grouping nodes in the euclidean space is similar to grouping nodes that are very connected to each other in the original network. \n",
    "\n",
    "In other words, the eigenvectors should naturally partition the graph into clusters such that node in a cluster are more connected to nodes within that cluster than to nodes outside of the cluster.\n",
    "\n",
    "We could use any other numerical values associated to the graph for clustering, so long they define a similarity mesure between the nodes. An example of such a feature matrix is the [topological overlap matrix](http://networksciencebook.com/chapter/9#hierarchical)\n",
    "\n",
    "In a broader sense of clustering, modularity can be used to find clusters (communities), as shown below, this gives quite interessting clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.from_numpy_array(adjacency[mask_largest_component > 0][:, mask_largest_component > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = list(greedy_modularity_communities(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We get {} clusters'.format(len(communities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i, c in enumerate(communities):\n",
    "    for member in c:\n",
    "        d[member] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity = pd.Series(d).sort_index().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors1 = plt.cm.Set1_r(0)\n",
    "colors2 = plt.cm.tab20b(list(range(20)))\n",
    "colors3 = plt.cm.tab20c(list(range(20)))\n",
    "\n",
    "# combine them and build a new colormap\n",
    "colors = np.vstack((colors1, colors2, colors3))\n",
    "mymap = mcolors.ListedColormap(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('communites (clusters) as given by modularity')\n",
    "plt.scatter(locations[:,1], locations[:,0], cmap=mymap, c=modularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
